# -*- coding: utf-8 -*-
"""ANOVAtesting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E911iO4iFXQbr5vSEtS4Seil1ZTV8VHJ

#Importing of packages
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
from sklearn import metrics

"""#This notebook includes 
1. Anova testing 
2. Post Hoc
3. Logistic Regression 
4. Kurtosis 
5. Shapiro testing

#Breast Cancer Wisconsin diagnostic Dataset
"""

from google.colab import files
uploaded = files.upload()

import io
import pandas as pd
df = pd.read_csv(io.BytesIO(uploaded['breast-cancer-wisconsin-data_data.csv']))

"""#Variables for dataframe that can be used for ANOVA testing
1. diagnosis Benign/ Malignant/ NAN/NUll (Unknown) D.V.
2. radius _ mean I.V.
3. area _ mean I.V.
4. radius _ se I.V.
5. area _ se I.V.
6. smoothness _ mean I.V.
7. smoothness _ se I.V.

#Load the dataframe 
dataframe = df
"""

df

"""#3 Countinous variables to be tested with correlation for diagnosis 
1. Radius mean
2. Area mean
3. Smoothness mean
"""

sns.boxplot(x = 'diagnosis', y = 'radius_mean', data = df)

sns.boxplot(x = 'diagnosis', y = 'area_mean', data = df)

sns.boxplot(x = 'diagnosis', y = 'smoothness_mean', data = df)

"""#Assumption Testing"""

diagnosis1 = df[df['diagnosis'] == 'B']
diagnosis2 = df[df['diagnosis'] == 'M']
diagnosis3 = df[df['diagnosis'] == 'NULL']

plt.hist(diagnosis1['radius_mean'])
plt.show()

plt.hist(diagnosis2['radius_mean'])
plt.show()

plt.hist(diagnosis3['radius_mean'])
plt.show()

"""#Anova Testing"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import statsmodels.formula.api as smf
import scipy.stats as stats

import statsmodels.api as sm

model = smf.ols("area_mean ~ C(diagnosis)", data = df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
anova_table

"""The difference between the area mean and diagnosis is significant due to the P value being less thn 0.05 allowing us to reject the null hypothesis."""

model = smf.ols("radius_mean ~ C(diagnosis)", data = df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
anova_table

"""The difference between the radius mean and diagnosis is significant due to the P value being less thn 0.05 allowing us to reject the null hypothesis."""

model = smf.ols("smoothness_mean ~ C(diagnosis)", data = df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
anova_table

"""The difference between the smoothness mean and diagnosis is significant due to the P value being less thn 0.05 allowing us to reject the null hypothesis.

#POST_HOC_RES
"""

df['diagnosis'].replace({'M':1, 'B':0}, inplace = True)

import statsmodels.stats.multicomp as mc

comp = mc.MultiComparison(df['diagnosis'], df['radius_mean'])
post_hoc_res = comp.tukeyhsd()
tukey1way = pd.DataFrame(post_hoc_res.summary())

print(comp)

print(tukey1way)

"""Based on the Tukey one way test their was no significant difference between group 1 and group 2 diagnosis and radius_mean.

"""

comp = mc.MultiComparison(df['diagnosis'], df['area_mean'])
post_hoc_res = comp.tukeyhsd()
tukey1way = pd.DataFrame(post_hoc_res.summary())

print(comp)

print(tukey1way)

"""Based on the Tukey one way test their was no significant difference between group 1 and group 2 diagnosis and radius_mean."""

comp = mc.MultiComparison(df['diagnosis'], df['smoothness_mean'])
post_hoc_res = comp.tukeyhsd()
tukey1way = pd.DataFrame(post_hoc_res.summary())

print(comp)

print(tukey1way)

"""Based on the Tukey one way their was no significant differences between group 1 and group 2 diagnosis and smoothness_mean."""

df.describe()

"""#KURTOSIS

# Kurtosis testing
 is used to measure if the data are heavy-tailed or   light-tailed relative to a normal distribution.That is, data sets with high 
 kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis 
 tend to have light tails, or lack of outliers.
 The dataset I used had three categories to be charted for diagnosis such as B, M, or NaN values below the kurtosis histograms for diagnosis 1,2, and 3 are 
show with area_mean. As we can see diagnosis 1 we saw .288 skewness level 
Which shows a low skew and a normal distrubution for benign tumors. Diagnosis  2 showed a skew greater than 1 and -2 at 2.22 meaning that the skew is  non-normal and is too peaked to be considered normal. Diagnosis 3 didnt show any results for missing values with NaN for diagnosis  values so no missing data was found for diagnosis in this dataset.
"""

from scipy.stats import kurtosis, skew, bartlett

# kertosis 
print(kurtosis(diagnosis1['area_mean']))
print(kurtosis(diagnosis2['area_mean']))
print(kurtosis(diagnosis3['area_mean']))

print('skew diagnosis1: ', skew(diagnosis1['area_mean']))
print('skew diagnosis2: ', skew(diagnosis2['area_mean']))
print(skew(diagnosis3['area_mean']))

"""#Logistic Regression

# Showed DF with mean and Se columns with diagnosis.
"""

df = df[['diagnosis','radius_mean','area_mean', 'radius_se', 'area_se', 'smoothness_mean','smoothness_se']]

df.head()

df.info()

"""# Boxplot shown below to see the correlation between the 2 variables and the 
# correlation.
"""

sns.boxplot(x = 'diagnosis', y = 'radius_mean', data = df)

"""# Check  how well the radius_mean can be utilized to classify, or separate the # datapoints in either B or M diagnosis.
# Selecting a boundary value for the radius mean and see how well it separates the data.
# This will take in a boundary value of our choosing and then classify the data points based on whether they are above or below the boundary.
"""

boundary = 10
sns.scatterplot(x = 'radius_mean', y = 'diagnosis', data = df)
plt.plot([boundary, boundary], [0, 1], 'g', linewidth = 6)

"""Down below I created a function that takes in a target boundary with thevalue of area_mean. 
The number I chose as the boundary was 17 due to the fact that most diagnosis 
for malignant tumors would greater or equal to 17 as the test/predictor model.

"""

def boundary_classifier(target_boundary,x):
  result = []
  for i in x:
    if i > target_boundary:
      result.append(1)
    else:
      result.append(0)
  return result

chosen_boundary = 17
y_pred = boundary_classifier(chosen_boundary, df['radius_mean'])
df['predicted'] = y_pred
y_true = df['diagnosis']
sns.scatterplot(x = 'radius_mean', y = 'diagnosis', hue = 'predicted', data = df)
plt.plot([chosen_boundary, chosen_boundary], [0, 1], 'g', linewidth = 6)

accuracy = metrics.accuracy_score(y_true,y_pred)
accuracy

"""The predictor model that I created for this dataset showed a .83 accuracy
score vs. the train model dataset.

#Here I created a subplot showing the Actual diagnosis vs. predicted diagnosis
"""

y_test = df['diagnosis']
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
class_names = [0,1]

fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names)) 
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y = 1.1)
plt.ylabel('Actual diagnosis')
plt.xlabel('Predicted diagnosis')

"""# Predictor model accuracy, precision, and recall scores for test model vs. 
# Train model.

"""

def model_stats(y_test, y_pred):
  print("Accuracy: ", metrics.accuracy_score(y_test, y_pred))
  print("Precision: ", metrics.precision_score(y_test, y_pred))
  print("Recall: ", metrics.recall_score(y_test, y_pred))

model_stats(y_test, y_pred)

from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df, test_size = 0.4, random_state = 1)

train_df.head()

test_df.head()

input_labels = ['radius_mean']
output_label = 'diagnosis'


x_train = train_df[input_labels]
print('Our x variables')
print(x_train.head())
print('\n\n')

y_train = train_df[output_label]
print('Our y variable:')
print(y_train.head())

from sklearn import linear_model
class_rm = linear_model.LogisticRegression()

class_rm = linear_model.LogisticRegression()
class_rm.fit(x_train, y_train)

x_test = test_df[input_labels]
y_test = test_df[input_labels]
y_test = test_df[output_label].values.squeeze()
y_pred = class_rm.predict(x_test)

print(y_pred)

"""#Visualization of the results"""

y_pred = y_pred.squeeze()
x_test_view = x_test[input_labels].values.squeeze()
sns.scatterplot(x = x_test_view, y = y_pred, hue = y_test)
plt.xlabel('radius_mean')
plt.ylabel('Predicted diagnosis')
plt.legend()

model_stats(y_test, y_pred)

y_prob = class_rm.predict_proba(x_test)
sns.scatterplot(x = x_test_view, y = y_prob[:,1], hue = y_test)

"""This logistic model gives a "line" with curvy ends in the [0,1] range, 
 meaning that is the best approximation for a line that will also always be 
within these boundaries and logistic regression is used for classification 
 problems where in this dataframe we are trying to classify patients tumors.

#Shapiro Test
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import statsmodels.formula.api as smf
import scipy.stats as stats

model = smf.ols("area_mean ~ C(diagnosis)", data = df).fit()
stats.shapiro(model.resid)

import matplotlib.pyplot as plt

fig = plt.figure(figsize= (10, 10))
ax = fig.add_subplot(111)

normality_plot, stat = stats.probplot(model.resid, plot= plt, rvalue= True)
ax.set_title("Probability plot of model residual's", fontsize= 20)
ax.set

plt.show()

""" The Shapiro-Wilks test for normality is a normality test
The test rejects the hypothesis of normality when the # p-value is less than 
 or equal to 0.05.
 As shown above the P value is greater than 0.05 meaning that the data is considered normal and we fail to reject the null hypthesis. 
"""